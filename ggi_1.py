"""ggi_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LifUQBEspkUycflvW_QMOlPDS-wu8ioy

# A neural network fit on DIS data

In these tutorials we will use a simple neural network to fit Deep Inelastic Scattering (DIS) data coming from the BCDMS experiment.

In doing so we will reproduce some of the results presented in this paper

[Neural network determination of parton distributions: The Nonsinglet case](https://inspirehep.net/literature/742505)

**Goals**

*   Implement a nn MC fit on BCDMS data
*   Implement training/validation split and cross validation
*   Validate the methodology using closure tests

## Part 1
### Input
There are a number of ingredients required to do this exercise


1.   Experimental data and full information regarding their uncertainty
2.   Fast Kernel tables
3.   A PDF set from the literature to check our results

In the first part of this notebook we will get familiar with these inputs and set the stage for the coming days
"""

# clone the git repo where the input is store
# !git clone https://github.com/tgiani/GGI_tutorials.git

"""## Data
We will consider BCDMS data for the difference bewteen proton and deuterium strcture function
$ F_2^p\\left(x,Q^2\right)-F_2^d\\left(x,Q^2\right) $
"""

import numpy as np

# load data central values
data = np.load("/content/GGI_tutorials/BCDMS_input/data.npy")

# load data covariance matrix
Cy = np.load("/content/GGI_tutorials/BCDMS_input/Cy.npy")

# load data kinematic info
kin = np.load("/content/GGI_tutorials/BCDMS_input/kin.npy")

# plot the kin coverage of the data

import matplotlib.pyplot as plt

plt.scatter(kin[:, 0], kin[:, 1] ** 2, marker="*", label="BCDMS $F_p-F_d$")
plt.xlabel("x")
plt.ylabel("$Q^2$ [GeV$^2$]")
plt.yscale("log")
plt.xscale("log")
plt.xlim([1e-3, 1.0])
plt.ylim([1, 1e3])
plt.legend()
plt.grid()
plt.show()

"""To have an idea of the kin coverage of the data enetering a real fit check pg 10 of [this paper](https://arxiv.org/pdf/2109.02653)

## Factorization
The observable
$$ F_2^p-F_2^d $$
is relaetd to the PDF
$$T_3\\left(x,\\mu^2\right) = u^+\\left(x,\\mu^2\right) - d^+\\left(x,\\mu^2\right)$$
through the convolution
$$ F_2^p\\left(x,Q^2\right)-F_2^d\\left(x,Q^2\right) = \\int_x^1 \frac{dy}{y}
C\\left(\frac{x}{y},\frac{Q^2}{\\mu_0^2}\right) T_3\\left(y,\\mu_0^2\right)$$

which holds up to power suppressed terms (the so-called higher twist).

The coefficient $C$ can be computed order by order in perturbation theory, and it encodes both the hard scattering and the PDF evolution from the reference scale $\\mu_0$ up to the hard process scale $Q$.
Our goal is to determine the x-dependence of $T_3\\left(x,\\mu_0^2\right)$ from the experimental data that we have loaded in the previous cell.

## Interpolation and FK tables
The convolution can be implemented numerically in the following way: we introduce an basis of interpolation functions

$$\\{I_{\alpha}\\left(x\right)\\,, \alpha=1, ... ,N_{\text{basis}}\\}$$

which allows us to write the PDF (up to a negligible interpolation error) as

$$T_3\\left(x\right) = \\sum_{\alpha} T_3\\left(x_{\alpha}\right) I_{\alpha}\\left(x\right)\\,.   $$

The set of points

$$\\{x_{\alpha}\\,, \alpha=1, ... ,N_{basis}\\}$$

is a given grid of x values which spans the interval $[0,1]$, which depends on the specific choice of interpolation basis. We can plug the expression of the PDF in terms of the interpolation basis in the convolution, and we get

$$ F_2^p-F_2^d = \\sum_{\alpha} \\left[\\int_x^1 \frac{dy}{y} C\\left(\frac{x}{y}\right) I_{\alpha}\\left(y\right)\right]
 T_3\\left(x_{\alpha}\right)  = \\sum_{\alpha} \text{FK}_{\alpha}\\,
 T_3\\left(x_{\alpha}\right)\\,. $$

 By using an interpolation basis we have expressed the convolution as a product between the matrix FK, called FK table and having shape $(N_{data}, N_{basis})$ and the vector $T_3\\left(x_{\alpha}\right)$, defined by the values of the PDF on a set of points.

 Therefore in order to compute theory predictions for an observable we need:
 * the corresponding FK table
 * the valued of the PDF on the points of the x-grid

### Exercise 1: data-theory comparison
Compute theory predictions for the BCDMS data using NNPDF4.0 and compare them with the corresponding exprimental error
"""

# load FK table
FK = np.load("/content/GGI_tutorials/BCDMS_input/FK.npy")

# load x-grid of the FK table
fk_grid = np.load("/content/GGI_tutorials/BCDMS_input/fk_grid.npy")

# load results from NNPDF4.0. This is a vector containing the values of T_3 from
# NNPDF4.0 on the x-grid loaded in the previous lines
f_ = np.load("/content/GGI_tutorials/BCDMS_input/NNPDF40.npy")
f = f_[6 * 50 : 7 * 50]

# Solution Ex 1
# compute theory prediction using NNPDF4.0 and compare them with the experimental data

yth = FK @ f

sigma = np.sqrt(np.diagonal(Cy)) / data
x = np.arange(yth.size)
ref = np.ones(yth.size)
plt.figure(figsize=(20, 5))
plt.errorbar(x, ref, sigma, alpha=0.5, label="data")
plt.scatter(x, yth / data, marker="*", c="red", label="theory predictions")
plt.ylim([0.1, 2.5])
plt.legend()

"""### Neural network

The following code is a simple implementation of a model using a neural network to represent the unknown PDF.

* First we implemet a standard linear layer.

* We then use a set of linear layers with a Relu activation function to model the PDF.

* Finally we define a layer implementing the convolution with the FK table.

Our model is then built by combining a PDF layer with a convolution layer.
This gives as output the theory prediction for the exprimental points as a function of the free parameters of the network.
It takes as input a vector of x values, corresponding to the x-values on which the PDF is defined (i.e. the FK table grid) and gives back the value of the observable
"""

# See https://keras.io/ for docs and examples

import tensorflow as tf
from tensorflow import keras


class Linear(keras.layers.Layer):
    def __init__(self, units=32):
        super().__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer="random_normal",
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer="random_normal",
            trainable=True,
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b


# pdf layer
class pdf_NN(keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.linear_1 = Linear(64)
        self.linear_2 = Linear(64)
        self.linear_3 = Linear(32)
        self.linear_4 = Linear(1)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.linear_2(x)
        x = tf.nn.relu(x)
        x = self.linear_3(x)
        x = tf.nn.relu(x)
        return self.linear_4(x)


# convolution layer
class ComputeConv(keras.layers.Layer):
    def __init__(self, FK):
        super().__init__()
        self.fk = tf.Variable(
            initial_value=tf.convert_to_tensor(FK, dtype="float32"),
            trainable=False,
        )

    def call(self, inputs):
        res = tf.tensordot(self.fk, inputs, axes=1)
        return res


# build the model
class Observable(keras.Model):
    """Combines the PDF and convolution into a model for training."""

    def __init__(
        self,
        FK,
        name="dis",
        **kwargs,
    ):
        super().__init__(name=name, **kwargs)
        self.pdf = pdf_NN()
        self.conv = ComputeConv(FK)

    def get_pdf(self, inputs):
        return self.pdf(inputs)

    def call(self, inputs):
        pdf_values = self.pdf(inputs)
        obs = self.conv(pdf_values)
        return obs


# an example:

# define an observable, giving as input the corresponding FK table
thpred = Observable(FK)

# the input x-values correspond to the x-points enetring the FK table.
# For the following you need to reshape the input vector from (n,) to (n,1)
x_input = fk_grid[:, np.newaxis]

# the value of the observable as a function of the free parameters of the net
# can be obatined as thpred(x_input). Try. What yoiu get is a set of values
# corresponding to a random initialization of the net
# thpred(x_input)

"""### Loss function
We will perform a Gaussian fit, which means that our loglikelihood is given by the $\\chi^2$ defined as


$$\\chi^2 = \\left(y-y^{th}\right)C_y^{-1}\\left(y-y^{th}\right)$$

### Exercise 2
* Implement the loss function.

  Note: to perform multiplication between different tensors use tf.tensordot.
  Make sure that the variables you use are cast in 'float32', which is what we will be using during the training

* to check that things are working, compute theory predictions using the model and compute the $\\chi^2$ with the experimental data
"""


# Solution Ex 2
def invert(Cy):
    l, u = tf.linalg.eigh(Cy)
    invCy = tf.cast(u @ tf.linalg.diag(1.0 / l) @ tf.transpose(u), "float32")
    invCy = u @ tf.linalg.diag(1.0 / l) @ tf.transpose(u)
    return tf.cast(invCy, "float32")


def chi2(y, yth, invCy):
    """Given a set of data with corrersponding th predictions and covariance matrix
    returns the value of the chi2
    """
    d = y - yth
    ndata = tf.cast(tf.size(y), "float32")
    res = tf.tensordot(d, tf.tensordot(invCy, d, axes=1), axes=1) / ndata
    return res


# are things working?
# build theory prediction for our observable,
# it s a function of the free parameters of the NN, which at this point are
# initialized randomly

invCy = invert(Cy)
chi2(data, thpred(x_input)[:, 0], invCy)

"""### Fit of the experimental data
In the following the look at the different steps enetring the fitting procedure
"""

# define the input and output x-grid. The input one correspond to the fk-grid,
# the output can be chosen freely and correpond to what will be sused to look
# at the resulting PDF. Can also be taken equal to the input grid

# input grid of the FK table
x_input = fk_grid[:, np.newaxis]

# output grid for the final pdf results
grid_smallx = np.geomspace(1e-6, 0.1, 30)
grid_largex = np.linspace(0.1, 1.0, 30)
x_output = np.concatenate([grid_smallx, grid_largex])[:, np.newaxis]

# define the model
thpred = Observable(FK)

# define the optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

# fix the numebr of epochs you want to use during the training
epochs = 500

train_chi2 = []
# implement the training loop
for epoch in range(epochs):
    with tf.GradientTape() as tape:
        obs = thpred(x_input)
        loss = chi2(data, obs[:, 0], invCy)

    # compute gradient wrt training parameters
    grads = tape.gradient(loss, thpred.trainable_weights)
    # update free parameters of the network
    optimizer.apply_gradients(zip(grads, thpred.trainable_weights))
    train_chi2.append(loss.numpy())

# now the model has been trained.
# We can get the corresponding PDF and use the x_output grid to plot it.
# We can plot NNPDF4.0 as well for reference.

pdf_result = thpred.get_pdf(x_output)
plt.plot(x_output, pdf_result.numpy() / x_output, c="green", alpha=0.5, label="fit")
plt.plot(x_input, f / x_input[:, 0], "--", c="black", label="NNPDF4.0")
plt.ylim([-1, 5])
plt.legend()

r"""### Exercise 3: plot the $\chi^2$ as a function of the training epoch"""


# Solution Ex 3
def plot_chi2_vs_epocs(chi2_res, epocs=500, label=r"$\chi^2$"):
    x = np.arange(epocs)
    plt.plot(x, chi2_res, label=label)
    plt.legend()
    # plt.show()


plot_chi2_vs_epocs(train_chi2)

"""If you run the code above different times, do you expect to get the exact same result? Why? Check your answer by running the code few times.

# Monte Carlo fit
The experimental data entering the analysis are affected by some experimental error, described by the covariance matrix Cy.

The Monte Carlo approach allows you to propagate such uncertainty in the final result for the PDF.

### Exercise 4


*   perform a fit on a gaussian fluctuation of the experimental data
*   run a MC fit of at least 10 replicas and plot the resulting distribution
*   produce a plot of the loss function during the training (loss function vs epoch number)
"""

# Solution Ex 4


def fit_replica(y, Cy, invCy, epochs, noise=False):
    thpred_NN = Observable(FK)
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

    if noise:
        yrep = tf.cast(np.random.multivariate_normal(y, Cy), "float32")
    else:
        yrep = tf.cast(y, "float32")

    # Iterate over epochs.
    train_chi2 = []
    for epoch in range(epochs):
        with tf.GradientTape() as tape:
            obs = thpred_NN(x_input)
            loss = chi2(yrep, obs[:, 0], invCy)

        grads = tape.gradient(loss, thpred_NN.trainable_weights)
        optimizer.apply_gradients(zip(grads, thpred_NN.trainable_weights))
        train_chi2.append(loss.numpy())

    return thpred_NN.get_pdf(x_output), np.asarray(train_chi2)


# run the fit
replicas_res = []
chi2_res = []

invCy = invert(Cy)

replicas = 10

for rep in range(replicas):
    print(f"fitting replica {rep}")
    res_, chi2_ = fit_replica(data, Cy, invCy, 500, noise=True)
    replicas_res.append(res_)
    chi2_res.append(chi2_)

# plot the results
for i in range(replicas):
    plt.plot(x_output, replicas_res[i].numpy() / x_output, c="green", alpha=0.5)
plt.ylim([-1, 5])
# plt.xscale('log')
plt.plot(x_input, f / x_input[:, 0], "--", c="black", label="NNPDF4.0")
plt.legend()

"""# Training/validation split and cross validation
### Exercise 5
*   modify the solution to the previous exercise by adding training and validations split
*   implement cross validation by using look-back: train your model for a fixed amount of epochs and pick as the final epoch for each replica the one giving the absolute minimum of the validation loss
*   plot the final results (cross validated replicas)
*   produce a plot showing training and validation loss vs epochs together with the epoch chosen by look-back
*   produce an histogram showing the selected best epoch across the replicas
"""


"""### Solutions"""
