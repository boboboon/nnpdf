"""ggi_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tMVVUkZwDBNRk52Pc6WAMCKGM1fvNOKW

# Validation of the methodology: closure tests

In this notebook we validate the methodology we developed in the previous notebooks, using closure tests and data space estimators.

**Goals**
*   ***Introducing different kind of closure tests:*** the general idea of a closure test is to perform a fit on a set of pseudo-data, which are generated using an input PDF, called ''the underlying law''. We want to see how well a given methodology is able to reconstruct the input PDF. There are different 3 different kinds of closure tests, which we are going to explore in the following.

*   ***Defining a faithful methodology in terms of baias and variance:*** a faithful methodology is a fitting methodlogy giving results which describe within the corresponding error both the input and the test data. We will introduce some estimators in data space which allow to address the problem in a quantitative way



**References**
1.  [Bayesian approach to inverse problems: an application to NNPDF
closure testing](https://inspirehep.net/files/61798ee2d0b17d7d41f21dce50247243)
2. [The path to proton structure at 1% accuracy](https://inspirehep.net/literature/1918284)
3.  [Parton distributions for the LHC Run II](https://arxiv.org/pdf/1410.8849.pdf)
"""

# !git clone https://github.com/tgiani/GGI_tutorials.git

# import BCDMS input
import matplotlib.pyplot as plt
import numpy as np

FK = np.load("/content/GGI_tutorials/BCDMS_input/FK.npy")
fk_grid = np.load("/content/GGI_tutorials/BCDMS_input/fk_grid.npy")
data = np.load("/content/GGI_tutorials/BCDMS_input/data.npy")
Cy = np.load("/content/GGI_tutorials/BCDMS_input/Cy.npy")
kin = np.load("/content/GGI_tutorials/BCDMS_input/kin.npy")
f_ = np.load("/content/GGI_tutorials/BCDMS_input/NNPDF40.npy")
f = f_[6 * 50 : 7 * 50]

import tensorflow as tf
from tensorflow import keras


class Linear(keras.layers.Layer):
    def __init__(self, units=32):
        super().__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer="random_normal",
            trainable=True,
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer="random_normal",
            trainable=True,
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b


# pdf layer
class pdf_NN(keras.layers.Layer):
    def __init__(self):
        super().__init__()
        self.linear_1 = Linear(64)
        self.linear_2 = Linear(64)
        self.linear_3 = Linear(32)
        self.linear_4 = Linear(1)

    def call(self, inputs):
        x = self.linear_1(inputs)
        x = tf.nn.relu(x)
        x = self.linear_2(x)
        x = tf.nn.relu(x)
        x = self.linear_3(x)
        x = tf.nn.relu(x)
        return self.linear_4(x)


# convolution layer
class ComputeConv(keras.layers.Layer):
    def __init__(self, FK):
        super().__init__()
        self.fk = tf.Variable(
            initial_value=tf.convert_to_tensor(FK, dtype="float32"),
            trainable=False,
        )

    def call(self, inputs):
        res = tf.tensordot(self.fk, inputs, axes=1)
        return res


# build the model
class Observable(keras.Model):
    """Combines the PDF and convolution into a model for training."""

    def __init__(
        self,
        FK,
        name="dis",
        **kwargs,
    ):
        super().__init__(name=name, **kwargs)
        self.pdf = pdf_NN()
        self.conv = ComputeConv(FK)

    def get_pdf(self, inputs):
        return self.pdf(inputs)

    def call(self, inputs):
        pdf_values = self.pdf(inputs)
        obs = self.conv(pdf_values)
        return obs


# input grid of the FK table
x_input = fk_grid[:, np.newaxis]
x_output = x_input


def invert(Cy):
    l, u = tf.linalg.eigh(Cy)
    invCy = tf.cast(u @ tf.linalg.diag(1.0 / l) @ tf.transpose(u), "float32")
    return invCy


def chi2(y, yth, invCy):
    """Given a set of data with corrersponding th predictions and covariance matrix
    returns the value of the chi2
    """
    # y=tf.cast(y,'float32')
    # yth=tf.cast(yth,'float32')
    d = y - yth
    ndata = tf.cast(tf.size(y), "float32")
    res = tf.tensordot(d, tf.tensordot(invCy, d, axes=1), axes=1) / ndata
    return res


def fit_replica(y, Cy, epochs, FK, noise=False, tv_split=True):
    thpred_NN = Observable(FK)
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

    # generate replica. Use real data
    if noise:
        yrep = np.random.multivariate_normal(y, Cy)
    else:
        yrep = y

    # split data into training and validation set.
    # the split is different for each replica
    n_data = y.shape[0]
    indices = np.random.permutation(n_data)
    if tv_split:
        training_idx, validation_idx = indices[: int(n_data / 2)], indices[int(n_data / 2) :]
    else:
        training_idx, validation_idx = indices[: int(n_data)], indices[int(n_data) :]

    yt, yv = yrep[training_idx], yrep[validation_idx]
    y_train = tf.cast(yt, "float32")
    y_validation = tf.cast(yv, "float32")

    Cyt = Cy[np.ix_(training_idx, training_idx)]
    Cyv = Cy[np.ix_(validation_idx, validation_idx)]

    Cy_inv = invert(Cy)
    Cyt_inv = invert(Cyt)
    Cyv_inv = invert(Cyv)

    train_chi2 = []
    validation_chi2 = []
    model_pdf = []

    # Iterate over epochs.
    for epoch in range(epochs):
        with tf.GradientTape() as tape:
            # compute theory prediction for all datapoints
            obs = thpred_NN(x_input)
            # select entries correpsonding to training set
            obs_train = tf.gather(obs, training_idx)
            # Compute training chi2
            loss = chi2(y_train, obs_train[:, 0], Cyt_inv)

        # compute validatiobn chi2
        obs_val = tf.gather(obs, validation_idx)
        loss_validation = chi2(y_validation, obs_val[:, 0], Cyv_inv)

        # perform optimizer step
        grads = tape.gradient(loss, thpred_NN.trainable_weights)
        optimizer.apply_gradients(zip(grads, thpred_NN.trainable_weights))

        # save train and validation chi2
        train_chi2.append(loss.numpy())
        validation_chi2.append(loss_validation.numpy())
        model_pdf.append(np.asarray(thpred_NN.get_pdf(x_output)))

    return model_pdf, train_chi2, validation_chi2


def plot_chi2_vs_epochs(rep, chi2_res, epochs=500, ylim=[1e-6, 2]):
    x = np.arange(epochs)
    plt.plot(x, chi2_res[rep], label=f"loss replica {rep}")
    plt.yscale("log")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.ylim(ylim)
    plt.legend()


"""# Level 0 closure test
In a level 0 CT we generate pseudo-data and fit them as they are, without adding any kind of fluctuation: we assume we know the data exactely and we fit them with a MonteCarlo approach.

Since the data are artificially generated from a given underlying law,
we know for sure that the problem has a solution (the underlying law we have used), i.e. there are no tensions or incompatibilities between the data we are fitting.

Then if our methodology is good enough we should see

1.   The loss function going to 0 with the numebr of epochs
2.   The underlying law being precisely reconstructed, at least in the data region

### Exercise 1


*   generate level 0 pseudo data using the input PDF loaded before and the FK table
*   perform a MC fit of ~10 replicas and plot the result. Is the underlying law well reconstructed?
*   what does the spread of the MC replicas represent?
*   produce a plot of the loss function vs the training epoch and check whether or not the loss function goes to 0
"""


"""# Level 1 closure test
In a level 1 closure test, instead of fitting the pseudo-data, we fit a gaussian fluctuation of them. The difference wrt the level 0 case is that now, given the fact that the data are noisy, there is not an exact solution of the problem. Note that, like in the level 0 case, all the replicas are fitted to the same data, there is no Gaussian fluctuation propagating the experimental data error.

### Exercise 2
Repeat the points of Exercise 1, but this time performing a level 1 closure test.

How does the loss function behaves with the epoch number now? Why?
"""


"""# Level 2 closure test
In a level 2 closure test we mimic the whole methodology: we fit level-1 data using a MonteCarlo approach. This time each PDF replica is fitted to a different data replica, obtained from the original level 1 data by adding a Gaussian noise (generated according to the experimental covariance matrix).

In this case we want to test the full methodology, so we activate training-validation split and cross validation.

### Exercise 3
Repeat the points of Exercise 1, 2, but this time performing a level 2 closure test.
"""


"""# Faithful methodologies

In the following we want to assess whether or not the methodology we are looking at is faithful.

### Theory-data comparison
First, we can produce a plot where we compare the theory predictions
we get from the level 2 closure test with the original level 0 data.

Assuming that the methodology is faithful,
the l0 data should fall within the error of the theory predictions computed with the level 2 results.
If this is the case it means that our methodology is able to describe correctly the true data even in the realistic situation in which these are noisy.

### Exercise 4
*   use the results of the level 2 CT to compute theory predictions for the BCDMS data and the corresponding theory (PDF) error
*   produce a plot comparing these theory predictions with the original level 0 data
*   are the theory predictions within the experimental error? What does this mean?
*   are the level 0 data within the theory (PDF) error? What does this mean?
"""


"""### Estimators in data space

A plot like the one of the previous point helps in visualizing the results and gives a qualitative understanding of how well our methodology is performing on the data enetring the fit.

Now we want ro address the following questions:
1.  How do we asses whether or not our methodology is faithful in a quantitative way?
2. How do we test how well the methodology generalize on a set of data not included in the analysis?



Denote as $y$ the vector of experimental data (level 0 data in this case), $y^{th}_k$ the corresponding vector of theory prediction obtained from the $k$-th replica and as $<y^{th}>$ its mean value across replicas
$$<y^{th}> = \frac{1}{N_{\text{rep}}}\\sum_{k=1}^{N_{rep}}\\,y^{th}_k\\,,$$


and define the following data space estimators

\begin{align}
&\text{bias} = \\left(<y^{th}>- y\right) C_y^{-1} \\left(<y^{th}>- y\right)\\,, \\
&\text{variance} = \frac{1}{N_{\text{rep}}}\\sum_{k=1}^{N_{rep}}\\,\\left(y_k -  <y^{th}>\right) C_y^{-1} \\left(y_k -<y^{th}>\right)\\,.
\\end{align}

In general, if one compute the ratio of these 2 estimators on a out-of-sample set,
a faithful methodology should give a value of order 1: this means that data not included in the fit, are still described by the results within the estimated theory error.

### Exercise 5

In order to assess whether or not the methodology is faithful, one should first split the data in a fitting and test (or out-of-sample) dataset. The former enters the fit while the latter is used at the end of the analysis to compute baias and variance.

*   What does baias and variance represent?
*   What does it mean if the ratio baias/variance $\\gg 1$ when computed on the test data? What if baias/variance $\\sim 1$?
*   Split the data into a fitting and out-of-sample dataset (1/2 and 1/2), generate l1 data and perform a level 2 closure test on the fitting dataset. Use the results to compute baias and variance on the test set. What is the value of their ratio? What does it mean? Is this enough to say whether or not the methodology is faithful? How can you get an uncertainty on the value of the baias/variance ratio?  
"""


"""### Solutions"""

## Solution Ex 1
# # generate pseudo-data using NNPDF4.0 and the FK tables
# y_l0 = FK@f

# replicas_l0 = []
# chi2_l0 = []

# replicas=5
# epochs=500

# for rep in range(replicas):

#     print(f'fitting replica {rep}')
#     res_, chi2_train, _ = fit_replica(y_l0, Cy, epochs, FK, noise=False, tv_split=False)
#     replicas_l0.append(res_)
#     chi2_l0.append(chi2_train)

# for i in range(0, replicas):
#     plt.plot(x_output,replicas_l0[i][epochs-1]/x_output,c='green', alpha=0.5)
# plt.ylim([-1,5])
# #plt.xscale('log')
# plt.plot(x_input, f/x_input[:,0],'--', c='black', label='NNPDF4.0')
# plt.legend()


# for rep in range(replicas):
#   plot_chi2_vs_epochs(rep, chi2_l0)

## Solution Ex 2
# replicas_l1 = []
# chi2_l1 = []

# y_l1 = np.random.multivariate_normal(y_l0,Cy)

# replicas=10
# epochs=500

# for rep in range(replicas):

#     print(f'fitting replica {rep}')
#     res_, chi2_train, _ = fit_replica(y_l1, Cy, epochs, FK, noise=False, tv_split=False)
#     replicas_l1.append(res_)
#     chi2_l1.append(chi2_train)

# for i in range(0, replicas):
#     plt.plot(x_output,replicas_l1[i][epochs-1]/x_output,c='green', alpha=0.5)
# plt.ylim([-1,5])
# #plt.xscale('log')
# plt.plot(x_input, f/x_input[:,0],'--', c='black', label='NNPDF4.0')
# plt.legend()


# for rep in range(replicas):
#   plot_chi2_vs_epochs(rep, chi2_l1, epochs, ylim=[0.5,1.8])

## Solution Ex 3
# replicas_l2 = []
# chi2_l2 = []
# chi2_val_l2 = []


# replicas=10
# epochs=500

# for rep in range(replicas):

#     print(f'fitting replica {rep}')
#     res_, chi2_train, chi2_val = fit_replica(y_l1, Cy, epochs, FK, noise=True)
#     replicas_l2.append(res_)
#     chi2_l2.append(chi2_train)
#     chi2_val_l2.append(chi2_val)

# best_epochs = []
# for rep in range(replicas):
#   best_epochs.append(np.argmin(chi2_val_l2[rep]))

# best_rep_l2 = []
# for rep, epoch in zip(replicas_l2, best_epochs):
#   best_rep_l2.append(rep[epoch])

# for i in range(0, replicas):
#     plt.plot(x_output,best_rep_l2[i]/x_output,c='green', alpha=0.5)
# plt.ylim([-1,5])
# #plt.xscale('log')
# plt.plot(x_input, f/x_input[:,0],'--', c='black', label='NNPDF4.0')
# plt.legend()

## Solution Ex 4
# yth_mean = np.mean(FK@np.asarray(best_rep_l2),axis=0)[:,0] # for this to make sense we would need more rplicas..
# yth_err = np.std(FK@np.asarray(best_rep_l2),axis=0)[:,0]

# sigma=np.sqrt(np.diagonal(Cy))/y_l0
# x = np.arange(yth_mean.size)
# ref = np.ones(yth_mean.size)
# plt.figure(figsize=(20,5))
# plt.errorbar(x, ref, sigma,alpha=0.5)
# plt.errorbar(x, yth_mean/y_l0, yth_err/y_l0, c='red', fmt='.')

## Solution Ex 5

# # create test and fitting data (1/2 and 1/2)
# # perform the fit on fitting data only
# n_data = y_l0.shape[0]
# indices = np.random.permutation(n_data)
# fitting_idx, test_idx = indices[:int(n_data/2)], indices[int(n_data/2):]


# yf, yt = y_l0[fitting_idx], y_l0[test_idx]
# y_fitting=tf.cast(yf,'float32')
# y_test=tf.cast(yt,'float32')

# Cyf = Cy[np.ix_(fitting_idx,fitting_idx)]
# Cyt = Cy[np.ix_(test_idx,test_idx)]

# Cyf_inv = invert(Cyf)
# Cyt_inv = invert(Cyt)

# FK_fitting = FK[fitting_idx,:]
# FK_test = FK[test_idx,:]


# def compute_b_v():
#   replicas_t = []
#   chi2_t = []
#   chi2_val_t = []


#   y_fitting_l1 = np.random.multivariate_normal(y_fitting,Cyf)

#   replicas=10
#   epochs=500

#   for rep in range(replicas):

#     print(f'fitting replica {rep}')
#     res_, chi2_train, chi2_val = fit_replica(y_fitting_l1, Cyf, epochs, FK_fitting, noise=True)
#     replicas_t.append(res_)
#     chi2_t.append(chi2_train)
#     chi2_val_t.append(chi2_val)

#   best_epochs_t = []
#   for rep in range(replicas):
#     best_epochs_t.append(np.argmin(chi2_val_t[rep]))

#   best_rep_t = []
#   for rep, epoch in zip(replicas_t, best_epochs_t):
#     best_rep_t.append(rep[epoch])

#   # th prediction for test data for each replica
#   y_th_rep = (FK_test@(np.asarray(best_rep_t)))[:,:,0]
#   # average th predictions across replicas for test data
#   y_th_avg = np.mean(y_th_rep,axis=0)

#   # baias on the test set
#   baias =  (y_th_avg - yt)@ np.asarray(Cyt_inv) @ (y_th_avg - yt)
#   # variance on the test set
#   variance = np.mean([(y_th_rep[i,:] - y_th_avg)@ np.asarray(Cyt_inv) @ (y_th_rep[i,:] - y_th_avg) for i in range(replicas)])

#   return baias/variance
